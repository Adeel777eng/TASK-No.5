{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adeel777eng/TASK-No.5/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "auto_tag_tickets.py\n",
        "\n",
        "Auto-tag support tickets:\n",
        "- Zero-shot classification using Hugging Face zero-shot pipeline (bart-large-mnli)\n",
        "- Optional few-shot using OpenAI ChatCompletion (requires OPENAI_API_KEY)\n",
        "- Fine-tune DistilBERT classifier and compare performance\n",
        "- Output top-3 tags per ticket for each method\n",
        "\n",
        "Usage:\n",
        "    python auto_tag_tickets.py --csv tickets.csv\n",
        "\n",
        "Optional args:\n",
        "    --tags tags.txt             # candidate tags (one per line). If omitted, tags are inferred.\n",
        "    --do_finetune               # perform fine-tuning (can be slow)\n",
        "    --use_openai_fewshot        # use OpenAI few-shot prompting (requires OPENAI_API_KEY env var)\n",
        "    --epochs 3\n",
        "    --batch_size 16\n",
        "    --output_dir saved_model\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import math\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from datasets import Dataset, DatasetDict # Removed load_metric\n",
        "import evaluate # Added evaluate library\n",
        "\n",
        "# Optional OpenAI few-shot (only used if requested and openai installed)\n",
        "try:\n",
        "    import openai\n",
        "    _HAS_OPENAI = True\n",
        "except Exception:\n",
        "    _HAS_OPENAI = False\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers\n",
        "# ---------------------------\n",
        "def load_data(csv_path: str) -> pd.DataFrame:\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if \"text\" not in df.columns:\n",
        "        raise ValueError(\"CSV must contain 'text' column.\")\n",
        "    return df\n",
        "\n",
        "def load_tags(tags_path: Optional[str], df: pd.DataFrame) -> List[str]:\n",
        "    if tags_path and os.path.exists(tags_path):\n",
        "        with open(tags_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            tags = [line.strip() for line in f if line.strip()]\n",
        "        if not tags:\n",
        "            raise ValueError(\"tags.txt is empty.\")\n",
        "        return tags\n",
        "    # Infer from labeled data if available\n",
        "    if \"label\" in df.columns and not df[\"label\"].isnull().all():\n",
        "        tags = sorted(df[\"label\"].dropna().unique().tolist())\n",
        "        return tags\n",
        "    raise ValueError(\"No tags provided and no labeled data to infer tags from. Provide tags.txt or labeled CSV.\")\n",
        "\n",
        "def prepare_datasets(df: pd.DataFrame, test_size=0.2, seed=42) -> Tuple[DatasetDict, List[str]]:\n",
        "    \"\"\"\n",
        "    Returns HuggingFace DatasetDict with train/val/test if labels exist; otherwise returns dataset for inference.\n",
        "    \"\"\"\n",
        "    if \"label\" in df.columns and not df[\"label\"].isnull().all():\n",
        "        # Only keep rows that have a label for supervised training\n",
        "        labeled_df = df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
        "        train_df, test_df = train_test_split(labeled_df, test_size=test_size, random_state=seed, stratify=labeled_df[\"label\"])\n",
        "        train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=seed, stratify=train_df[\"label\"])\n",
        "        ds = DatasetDict({\n",
        "            \"train\": Dataset.from_pandas(train_df[[\"text\", \"label\"]]),\n",
        "            \"validation\": Dataset.from_pandas(val_df[[\"text\", \"label\"]]),\n",
        "            \"test\": Dataset.from_pandas(test_df[[\"text\", \"label\"]]),\n",
        "        })\n",
        "        labels = sorted(labeled_df[\"label\"].unique().tolist())\n",
        "        return ds, labels\n",
        "    else:\n",
        "        # No labels â€” just return dataset for inference\n",
        "        ds = Dataset.from_pandas(df[[\"text\"]])\n",
        "        return DatasetDict({\"infer\": ds}), []\n",
        "\n",
        "# ---------------------------\n",
        "# Zero-shot classification\n",
        "# ---------------------------\n",
        "def run_zero_shot(texts: List[str], candidate_labels: List[str], hypothesis_template: str = \"This example is {}.\") -> List[List[Tuple[str, float]]]:\n",
        "    \"\"\"\n",
        "    Returns top-k label probabilities per text as list of (label, score) tuples sorted desc.\n",
        "    \"\"\"\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    zsp = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
        "    results = []\n",
        "    for t in tqdm(texts, desc=\"zero-shot\"):\n",
        "        out = zsp(t, candidate_labels, hypothesis_template=hypothesis_template)\n",
        "        labels = out[\"labels\"]\n",
        "        scores = out[\"scores\"]\n",
        "        results.append(list(zip(labels, scores)))\n",
        "    return results\n",
        "\n",
        "# ---------------------------\n",
        "# Optional few-shot using OpenAI (chat)\n",
        "# ---------------------------\n",
        "def run_openai_fewshot(texts: List[str], candidate_labels: List[str], examples: List[Tuple[str, str]], top_k=3) -> List[List[Tuple[str, float]]]:\n",
        "    \"\"\"\n",
        "    Few-shot with OpenAI ChatCompletion. Requires OPENAI_API_KEY in env and openai package.\n",
        "    examples: list of (text, label) training exemplars to include in prompt\n",
        "    Returns top_k predicted labels with dummy scores (OpenAI doesn't return probs reliably).\n",
        "    \"\"\"\n",
        "    if not _HAS_OPENAI:\n",
        "        raise RuntimeError(\"openai package not installed. Install openai to use few-shot mode.\")\n",
        "    if \"OPENAI_API_KEY\" not in os.environ:\n",
        "        raise RuntimeError(\"OPENAI_API_KEY not found in environment for OpenAI few-shot.\")\n",
        "\n",
        "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "    system_prompt = \"You are an assistant that assigns a single tag from the provided candidate tags to each support ticket. Return a JSON list of top tags with confidences.\"\n",
        "\n",
        "    # craft few-shot prompt\n",
        "    example_block = \"\"\n",
        "    for ex_text, ex_label in examples:\n",
        "        example_block += f\"Ticket: {ex_text}\\nTag: {ex_label}\\n\\n\"\n",
        "\n",
        "    candidate_block = \"Candidate tags: \" + \", \".join(candidate_labels) + \"\\n\\n\"\n",
        "\n",
        "    # Fix: Correctly format the prompt string to avoid SyntaxError\n",
        "    prompt = f\"{system_prompt}\\n\\n{candidate_block}Examples:\\n{example_block}Now, label the following ticket. Provide the top {top_k} tags in JSON as [{{\\\"tag\\\": <tag>, \\\"score\\\": <0-1>}}].\\n\\nTicket: \" + text + \"\\n\\nAnswer:\"\n",
        "\n",
        "    results = []\n",
        "    for text in tqdm(texts, desc=\"openai-fewshot\"):\n",
        "        # Use ChatCompletion (gpt-3.5-turbo)\n",
        "        try:\n",
        "            resp = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=200,\n",
        "            )\n",
        "            content = resp[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "            # Attempt to parse JSON inside content; fallback to simple heuristics\n",
        "            import json, re\n",
        "            m = re.search(r\"\\[.*\\]\", content, re.S)\n",
        "            parsed = []\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(0))\n",
        "                except Exception:\n",
        "                    # crude parse: find tags in text\n",
        "                    for tag in candidate_labels:\n",
        "                        if tag.lower() in content.lower():\n",
        "                            parsed.append({\"tag\": tag, \"score\": 0.9})\n",
        "            else:\n",
        "                # fallback: pick first mentioned tags in candidate_labels\n",
        "                parsed = []\n",
        "                for tag in candidate_labels:\n",
        "                    if tag.lower() in content.lower():\n",
        "                        parsed.append({\"tag\": tag, \"score\": 0.9})\n",
        "            # normalize to list of tuples\n",
        "            tup = []\n",
        "            for p in parsed[:top_k]:\n",
        "                tag = p.get(\"tag\") if isinstance(p, dict) else str(p)\n",
        "                score = float(p.get(\"score\", 0.5)) if isinstance(p, dict) else 0.5\n",
        "                tup.append((tag, score))\n",
        "            results.append(tup)\n",
        "        except Exception as e:\n",
        "            results.append([(candidate_labels[0], 1.0)])  # fallback\n",
        "    return results\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Fine-tune classifier (DistilBERT)\n",
        "# ---------------------------\n",
        "def finetune_classifier(ds: DatasetDict, label_list: List[str], output_dir: str, epochs=3, batch_size=16, lr=2e-5) -> Tuple[str, AutoTokenizer, AutoModelForSequenceClassification]:\n",
        "    \"\"\"\n",
        "    Fine-tune DistilBERT on ds['train'] with ds['validation'].\n",
        "    Returns model_dir path, tokenizer and model.\n",
        "    \"\"\"\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    tokenized = ds.map(tokenize, batched=True, remove_columns=ds[\"train\"].column_names)\n",
        "\n",
        "    # map labels to ids\n",
        "    label2id = {l:i for i,l in enumerate(label_list)}\n",
        "    def label_map(batch):\n",
        "        batch[\"labels\"] = [label2id[l] for l in batch[\"label\"]]\n",
        "        return batch\n",
        "    tokenized = tokenized.map(label_map, batched=True)\n",
        "\n",
        "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=lr,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "        push_to_hub=False,\n",
        "    )\n",
        "\n",
        "    # metrics\n",
        "    # Load metric using evaluate.load()\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        # Compute metrics using the loaded metric objects\n",
        "        acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "        f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "        return {\"accuracy\": acc, \"f1_macro\": f1}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    return output_dir, tokenizer, model\n",
        "\n",
        "def predict_finetuned(model_dir: str, texts: List[str], top_k=3) -> List[List[Tuple[str, float]]]:\n",
        "    \"\"\"\n",
        "    Load finetuned model and produce top_k label probabilities per text.\n",
        "    \"\"\"\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    classifier = pipeline(\"text-classification\", model=model_dir, return_all_scores=True, device=device, function_to_apply=None)\n",
        "    results = []\n",
        "    for t in tqdm(texts, desc=\"finetuned-predict\"):\n",
        "        out = classifier(t)[0]  # list of dicts\n",
        "        # out contains dicts with 'label' and 'score'\n",
        "        # ensure sorted\n",
        "        out_sorted = sorted([(d[\"label\"], d[\"score\"]) for d in out], key=lambda x: x[1], reverse=True)\n",
        "        results.append(out_sorted[:top_k])\n",
        "    return results\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities: evaluate top-1 accuracy & macro F1 for predicted lists\n",
        "# ---------------------------\n",
        "def evaluate_top1(preds_topk: List[List[Tuple[str, float]]], golds: List[str]) -> Dict[str, float]:\n",
        "    # preds_topk: list of list[(label, score)]\n",
        "    top1 = [p[0][0] if p and len(p)>0 else \"\" for p in preds_topk]\n",
        "    acc = accuracy_score(golds, top1)\n",
        "    f1 = f1_score(golds, top1, average=\"macro\")\n",
        "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
        "\n",
        "# ---------------------------\n",
        "# Main CLI\n",
        "# ---------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--csv\", type=str, default=\"tickets.csv\", help=\"CSV with 'text' and optionally 'label'\")\n",
        "    parser.add_argument(\"--tags\", type=str, default=None, help=\"Optional tags.txt file\")\n",
        "    parser.add_argument(\"--do_finetune\", action=\"store_true\", help=\"Perform fine-tuning\")\n",
        "    parser.add_argument(\"--use_openai_fewshot\", action=\"store_true\", help=\"Use OpenAI few-shot\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"saved_model\")\n",
        "    # Use parse_known_args() to ignore extra arguments from the Colab kernel\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    csv_path = args.csv\n",
        "    # Check for the presence of the CSV file\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"CSV file not found: {csv_path}. Please ensure the CSV file is in the correct directory.\")\n",
        "        # You might want to exit or handle this differently depending on desired behavior\n",
        "        return # Exit the main function if the file is not found\n",
        "\n",
        "\n",
        "    df = load_data(args.csv)\n",
        "    print(f\"Loaded {len(df)} rows from {args.csv}\")\n",
        "\n",
        "    # Prepare datasets (if labels exist)\n",
        "    ds_dict, inferred_labels = prepare_datasets(df)\n",
        "    # candidate tags\n",
        "    tags = load_tags(args.tags, df) if args.tags or inferred_labels else inferred_labels\n",
        "    print(\"Candidate tags:\", tags)\n",
        "\n",
        "    # TEXTS to predict â€” use full CSV order\n",
        "    texts = df[\"text\"].astype(str).tolist()\n",
        "\n",
        "    # 1) Zero-shot\n",
        "    print(\"\\n=== Running zero-shot classification ===\")\n",
        "    zs_results = run_zero_shot(texts, tags)\n",
        "    # print top-3 for first 5\n",
        "    for i, (txt, res) in enumerate(zip(texts[:5], zs_results[:5])):\n",
        "        print(f\"\\nTicket: {txt}\\nZero-shot top-3:\")\n",
        "        for label, score in res[:3]:\n",
        "            print(f\"  {label}: {score:.3f}\")\n",
        "\n",
        "    # Evaluate if gold labels exist\n",
        "    if \"label\" in df.columns and not df[\"label\"].isnull().all():\n",
        "        golds = df[\"label\"].fillna(\"\").tolist()\n",
        "        zs_eval = evaluate_top1(zs_results, golds)\n",
        "        print(\"\\nZero-shot evaluation (top-1):\", zs_eval)\n",
        "\n",
        "    # 2) Optional few-shot via OpenAI\n",
        "    if args.use_openai_fewshot:\n",
        "        if not _HAS_OPENAI or \"OPENAI_API_KEY\" not in os.environ:\n",
        "            print(\"OpenAI few-shot requested but openai package or OPENAI_API_KEY not available. Skipping.\")\n",
        "        else:\n",
        "            # Build small example set from labeled rows (up to 5 per tag)\n",
        "            examples = []\n",
        "            if \"label\" in df.columns and not df[\"label\"].isnull().all():\n",
        "                sampled = df.dropna(subset=[\"label\"]).groupby(\"label\").head(2)  # 2 examples per label (few-shot)\n",
        "                examples = list(zip(sampled[\"text\"].tolist(), sampled[\"label\"].tolist()))\n",
        "            else:\n",
        "                # if no labels, craft synthetic screenshots or prompt-less few-shot is weaker\n",
        "                examples = [(texts[i], tags[0]) for i in range(min(4, len(texts)))]\n",
        "            print(\"\\n=== Running OpenAI few-shot classification (may consume credits) ===\")\n",
        "            fs_results = run_openai_fewshot(texts, tags, examples, top_k=3)\n",
        "            for txt, res in zip(texts[:5], fs_results[:5]):\n",
        "                print(f\"\\nTicket: {txt}\\nFew-shot top predictions: {res}\")\n",
        "\n",
        "            if \"label\" in df.columns and not df[\"label\"].isnull().all():\n",
        "                fs_eval = evaluate_top1(fs_results, df[\"label\"].fillna(\"\").tolist())\n",
        "                print(\"\\nFew-shot evaluation (top-1):\", fs_eval)\n",
        "\n",
        "    # 3) Fine-tune (optional)\n",
        "    if args.do_finetune:\n",
        "        if not inferred_labels:\n",
        "            raise ValueError(\"Fine-tuning requires labeled data (a 'label' column).\")\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "        print(\"\\n=== Fine-tuning classifier ===\")\n",
        "        model_dir, tokenizer, model = finetune_classifier(ds_dict, inferred_labels, output_dir=args.output_dir, epochs=args.epochs, batch_size=args.batch_size)\n",
        "        print(\"Fine-tuned model saved to:\", model_dir)\n",
        "\n",
        "        # Predictions with fine-tuned model\n",
        "        ft_results = predict_finetuned(model_dir, texts, top_k=3)\n",
        "        for txt, res in zip(texts[:5], ft_results[:5]):\n",
        "            print(f\"\\nTicket: {txt}\\nFine-tuned top-3:\")\n",
        "            for label, score in res:\n",
        "                print(f\"  {label}: {score:.3f}\")\n",
        "\n",
        "        if \"label\" in df.columns and not df[\"label\"].isnull().all():\n",
        "            ft_eval = evaluate_top1(ft_results, df[\"label\"].fillna(\"\").tolist())\n",
        "            print(\"\\nFine-tuned evaluation (top-1):\", ft_eval)\n",
        "\n",
        "    # Save combined outputs to CSV\n",
        "    print(\"\\nSaving top-3 predictions to 'predictions_output.csv' ...\")\n",
        "    out = df.copy()\n",
        "    # zero-shot top-3 strings\n",
        "    out[\"zs_top3\"] = [\";\".join([f\"{l}|{s:.3f}\" for l,s in res[:3]]) for res in zs_results]\n",
        "    if args.do_finetune:\n",
        "        out[\"ft_top3\"] = [\";\".join([f\"{l}|{s:.3f}\" for l,s in res[:3]]) for res in ft_results]\n",
        "    else:\n",
        "        out[\"ft_top3\"] = \"\"\n",
        "    out.to_csv(\"predictions_output.csv\", index=False)\n",
        "    print(\"Saved predictions_output.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Use parse_known_args() to ignore extra arguments from the Colab kernel\n",
        "    parser = argparse.ArgumentParser() # Re-initialize the parser\n",
        "    parser.add_argument(\"--csv\", type=str, default=\"tickets.csv\", help=\"CSV with 'text' and optionally 'label'\")\n",
        "    parser.add_argument(\"--tags\", type=str, default=None, help=\"Optional tags.txt file\")\n",
        "    parser.add_argument(\"--do_finetune\", action=\"store_true\", help=\"Perform fine-tuning\")\n",
        "    parser.add_argument(\"--use_openai_fewshot\", action=\"store_true\", help=\"Use OpenAI few-shot\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"saved_model\")\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    # Pass the parsed arguments to main\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "YykPnN4wbrSj",
        "outputId": "7f22b8ea-13da-44d6-9254-5b5cdc6e3018"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'evaluate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1965226996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetDict\u001b[0m \u001b[0;31m# Removed load_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0;31m# Added evaluate library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Optional OpenAI few-shot (only used if requested and openai installed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}